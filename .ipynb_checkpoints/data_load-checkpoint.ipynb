{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import scipy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import flow_transforms\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread\n",
    "\n",
    "def load_flo(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        magic = np.fromfile(f, np.float32, count=1)\n",
    "        assert(202021.25 == magic),'Magic number incorrect. Invalid .flo file'\n",
    "        h = np.fromfile(f, np.int32, count=1)[0]\n",
    "        w = np.fromfile(f, np.int32, count=1)[0]\n",
    "        data = np.fromfile(f, np.float32, count=2*w*h)\n",
    "    # Reshape data into 3D array (columns, rows, bands)\n",
    "    data2D = np.resize(data, (w, h, 2))\n",
    "    return data2D\n",
    "\n",
    "def default_loader(root, path_imgs, path_flo, path_occ):\n",
    "    imgs = [os.path.join(root,path) for path in path_imgs]\n",
    "    flo = os.path.join(root,path_flo)\n",
    "    occ = os.path.join(root,path_occ)\n",
    "    return [imread(img).astype(np.float32) for img in imgs], load_flo(flo), imread(occ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "for flow_map in sorted(glob.glob(os.path.join('sintel','flow','ambush_4','*.flo'))):\n",
    "    flow_map = os.path.relpath(flow_map, os.path.join('sintel','flow'))\n",
    "    \n",
    "    scene_dir, filename = os.path.split(flow_map)\n",
    "    no_ext_filename = os.path.splitext(filename)[0]\n",
    "    prefix, frame_nb = no_ext_filename.split('_')\n",
    "    frame_nb = int(frame_nb)\n",
    "    \n",
    "    img1 = os.path.join('clean', scene_dir, '{}_{:04d}.png'.format(prefix, frame_nb))\n",
    "    img2 = os.path.join('clean', scene_dir, '{}_{:04d}.png'.format(prefix, frame_nb+1))\n",
    "    occ_mask = os.path.join('occlusions', scene_dir, '{}_{:04d}.png'.format(prefix, frame_nb))\n",
    "    flow_map = os.path.join('flow', flow_map)\n",
    "    \n",
    "    if not (os.path.isfile(os.path.join('sintel',img1)) and os.path.isfile(os.path.join('sintel',img2))):\n",
    "        continue\n",
    "    images.append([[img1,img2], flow_map, occ_mask])\n",
    "\n",
    "split_factor = 1.1     ## ratio of train split\n",
    "split_values = np.random.uniform(0,1,len(images)) < split_factor\n",
    "train_samples = [sample for sample, split in zip(images, split_values) if split]\n",
    "test_samples = [sample for sample, split in zip(images, split_values) if not split]\n",
    "\n",
    "#print(train_samples[0], test_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root = 'sintel'\n",
    "#index = 8\n",
    "\n",
    "#inputs, target, mask = train_samples[index] # or test_samples (path_list)\n",
    "#print(mask)\n",
    "\n",
    "#import matplotlib.image as mpimg\n",
    "#img1 = mpimg.imread(os.path.join(root, inputs[0]))\n",
    "#img2 = mpimg.imread(os.path.join(root, inputs[1]))\n",
    "#mask = mpimg.imread(os.path.join(root, mask))\n",
    "#mask = np.repeat(np.expand_dims(mask, axis=2), 3, axis=2)\n",
    "#print(img1.max())\n",
    "\n",
    "#fig = plt.figure()\n",
    "#a = fig.add_subplot(3, 1, 1)\n",
    "#imgplot = plt.imshow(img1)\n",
    "#a = fig.add_subplot(3, 1, 2)\n",
    "#imgplot = plt.imshow(img2)\n",
    "#a = fig.add_subplot(3, 1, 3)\n",
    "#imgplot = plt.imshow(mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(66051.)\n",
      "tensor(66080.9062)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(52599.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(48545.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(32232.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(48968.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(68742.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(44724.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n",
      "tensor(33190.)\n",
      "tensor(0.)\n",
      "tensor(380383.0625)\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "start_frame = 15\n",
    "end_frame = 23\n",
    "\n",
    "inputs, target, mask = train_samples[start_frame]\n",
    "inputs, target, mask = default_loader(root, inputs, target, mask)\n",
    "target = torch.from_numpy(target)\n",
    "\n",
    "width = target.size(1)\n",
    "height = target.size(0)\n",
    "\n",
    "## Grid Location Point (x,y) matrix of size H X W\n",
    "X = torch.Tensor([[range(height)]*width]).transpose(1,2)\n",
    "Y = torch.Tensor([[range(width)]*height])\n",
    "Grid = torch.cat([X,Y], 0).permute(1, 2, 0)\n",
    "\n",
    "offset = torch.zeros(height, width, 2)     ## sum of flows from start to end frame.  H x W x 2\n",
    "source_grid = torch.ones(height, width)    ## Survived points in start frame image.  H x W\n",
    "\n",
    "new_Grid = Grid.clone()\n",
    "for ind in range(start_frame, end_frame):\n",
    "    _, target, mask = train_samples[ind]\n",
    "    target = torch.from_numpy(load_flo(os.path.join(root, target)))                  ## Flow annotation. H x W x 2\n",
    "    mask = torch.from_numpy(imread(os.path.join(root, mask))).float() / 255          ## Occlusion mask. H x W\n",
    "    print(mask.sum())  \n",
    "    #new_Grid[:,:,0] = torch.clamp((new_Grid + target.int())[:,:,0], 0, height-1)\n",
    "    #new_Grid[:,:,1] = torch.clamp((new_Grid + target.int())[:,:,1], 0, width-1)\n",
    "    \n",
    "    new_Grid[:,:,0] = 2.0*new_Grid[:,:,0].clone() / max(height-1,1)-1.0\n",
    "    new_Grid[:,:,1] = 2.0*new_Grid[:,:,1].clone() / max(width-1,1)-1.0\n",
    "    \n",
    "    new_mask = nn.functional.grid_sample(mask[(None,)*2 + (...,)], new_Grid.unsqueeze(0), align_corners=True)\n",
    "    print(new_mask.sum())\n",
    "    source_grid = source_grid * (1 - new_mask[0][0])\n",
    "    print(source_grid.sum())\n",
    "    print('----------------')\n",
    "\n",
    "outputs, target, mask = train_samples[end_frame]    \n",
    "outputs, _, mask = default_loader(root, outputs, target, mask)\n",
    "\n",
    "source_points = (source_grid == 1).nonzero().transpose(0,1)\n",
    "target_points = new_Grid[source_points[0], source_points[1]].long().transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "#################   Visualize Matches   #################\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 368318])\n",
      "torch.Size([2, 368318])\n",
      "torch.Size([436, 1024])\n",
      "torch.Size([1, 436, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(target_points.size())\n",
    "print(source_points.size())\n",
    "print(source_grid.size())\n",
    "print(new_mask[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-81ebc871604b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_composite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Workspace/pytorch-pwc/utils.py\u001b[0m in \u001b[0;36mbuild_composite_image\u001b[0;34m(image_path1, image_path2, axis, margin, background)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mim2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mh1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mh2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from utils import build_composite_image\n",
    "import cv2\n",
    "img1 = os.path.join(root, train_samples[start_frame][0][0])\n",
    "img2 = os.path.join(root, train_samples[end_frame][0][0])\n",
    "\n",
    "im, v_offset, h_offset = build_composite_image(img1, img2, margin=5, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(im)\n",
    "linewidth = 0.5\n",
    "\n",
    "for idx in range(int(source_point.size(1)/6)):\n",
    "    plt.plot((source_point[0,idx] + h_offset[0], target_point[0,idx] + h_offset[1]),\n",
    "             (source_point[1,idx] + v_offset[0], target_point[1,idx] + v_offset[1]),\n",
    "             color='r',\n",
    "             linewidth=linewidth)\n",
    "plt.tight_layout()\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "################   Image Preprocessing   ################\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### Data Pre-processing #######\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    flow_transforms.ArrayToTensor(),\n",
    "    transforms.Normalize(mean=[0,0,0], std=[255,255,255]),\n",
    "    transforms.Normalize(mean=[0.45,0.432,0.411], std=[1,1,1])\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "    flow_transforms.ArrayToTensor(),\n",
    "    transforms.Normalize(mean=[0,0],std=[20,20])\n",
    "])\n",
    "## sparse ##\n",
    "co_transform = flow_transforms.Compose([\n",
    "    flow_transforms.RandomCrop((320,448)),\n",
    "    flow_transforms.RandomVerticalFlip(),\n",
    "    flow_transforms.RandomHorizontalFlip()\n",
    "])\n",
    "\n",
    "print(inputs[0].shape, target.shape)   ### Compare before & after data pre-processing ###\n",
    "if co_transform is not None:\n",
    "    inputs, target = co_transform(inputs, target)\n",
    "if input_transform is not None:\n",
    "    inputs[0] = input_transform(inputs[0])\n",
    "    inputs[1] = input_transform(inputs[1])\n",
    "if target_transform is not None:\n",
    "    target = target_transform(target)\n",
    "\n",
    "print(inputs[0][0,100:107,100:107])\n",
    "print(target[0].max(), target[1].max())\n",
    "print(inputs[0].size(), target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pwcnet",
   "language": "python",
   "name": "pwcnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
